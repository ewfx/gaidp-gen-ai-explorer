from flask import Flask, request, jsonify,send_file
import pandas as pd
import os
import google.generativeai as genai
import logging
from flask_cors import CORS
import json  # Add this at the top
import re
from rapidfuzz import fuzz, process
import json
import csv

# Configure logging
logging.basicConfig(
    filename="flask_app.log",  # Log file name
    level=logging.INFO,  # Log level (DEBUG, INFO, WARNING, ERROR)
    format="%(asctime)s - %(levelname)s - %(message)s"
)

app = Flask(__name__)
CORS(app)
# Configure Gemini API
genai.configure(api_key="AIzaSyDGHu1_exPZmOuvqvnjZyjMa5ve9v8tSbQ")
model = genai.GenerativeModel("gemini-1.5-pro")

def get_excel_data(file_path):
    try:
        if file_path.endswith('.csv'):
            try:
                # Detect delimiter
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
                    sample = file.read(1024)  # Read a small chunk to analyze
                    file.seek(0)  # Reset pointer
                    try:
                        dialect = csv.Sniffer().sniff(sample)
                        delimiter = dialect.delimiter
                    except csv.Error:
                        delimiter = ','  # Default to comma if detection fails
                df = pd.read_csv(file_path,encoding='utf-8', engine='python')
                return df.columns.tolist(), df.sample(min(10, len(df))).to_dict(
                    orient='records')  # Sample data (max 10 rows)
            except UnicodeDecodeError:
                print("⚠ UnicodeDecodeError: Retrying with 'ISO-8859-1' encoding...")
                df = pd.read_csv(file_path,sheet_name=0, encoding='ISO-8859-1', engine='python', delimiter=delimiter)
        elif file_path.endswith(('.xls', '.xlsx', '.csv')):
            try:
                df = pd.read_excel(file_path,sheet_name=0, engine='openpyxl')  # For XLSX files
                return df.columns.tolist(), df.sample(min(10, len(df))).to_dict(
                    orient='records')  # Sample data (max 10 rows)
            except ImportError:
                print("⚠ 'openpyxl' not found! Trying 'xlrd' for older Excel formats...")
                df = pd.read_excel(file_path,sheet_name=0, engine='xlrd')  # For older XLS files
        else:
            raise ValueError("❌ Unsupported file format. Please provide a CSV or Excel file.")
    except Exception as e:
        print(f"❌ Error reading file: {e}")
        return None

def generate_rules_gemini(columns, sample_data, num_rules):
    """Use Gemini LLM to generate validation rules based on column names and sample data."""
    prompt = f"""
    Generate exactly {num_rules} unique validation rules for the dataset.
    Columns: {', '.join(columns)}
    Sample Data: {sample_data}
    Respond strictly in valid JSON format as:
    [
        {{"Rule Name": "<Rule Title>", "Description": "<Validation Logic>"}},
        ...
    ]
    """
    response = model.generate_content(prompt)

    if response and response.candidates:
        try:
            # Extract JSON text from response
            rules_text = response.candidates[0].content.parts[0].text.strip()

            # Ensure we remove any leading markdown indicators (like ```json)
            if rules_text.startswith("```json"):
                rules_text = rules_text[7:].strip("```")

            rules = json.loads(rules_text)  # Parse JSON
            return rules if isinstance(rules, list) else []
        except json.JSONDecodeError as e:
            print(f"JSON parsing error: {e}")  # Debugging
            return []

    return []


@app.route('/generate_rules', methods=['POST'])
def generate_rules_api():
    try:
        # Read parameters
        request_data = request.json
        num_rules = int(request_data.get('ruleCount', 5))
        file_path = request_data.get('excelTemplate')

        logging.info(f"Received request: num_rules={num_rules}, file_path={file_path}")

        if not file_path or not os.path.exists(file_path):
            logging.error("File not found error.")
            return jsonify({"error": "File not found"}), 400

        # Extract column names and sample data
        columns, sample_data = get_excel_data(file_path)

        # Generate rules using Gemini LLM
        rules = generate_rules_gemini(columns, sample_data, num_rules)

        if not rules:
            logging.warning("No rules generated by Gemini.")
            return jsonify({"error": "Failed to generate rules"}), 500

        # Save rules to Excel
        rules_df = pd.DataFrame(rules)

        with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
            rules_df.to_excel(writer, sheet_name='rules', index=False)

        logging.info(f"Successfully generated {len(rules)} rules.")

        return jsonify({"message": "Rules generated successfully", "rules_count": len(rules), "rules": rules})

    except Exception as e:
        logging.error(f"Unexpected error: {str(e)}")
        return jsonify({"error": str(e)}), 500


@app.route('/filter_data', methods=['GET'])
def filter_data_api():
    try:

        file_path = request.args.get('file')
        if not file_path:
            return jsonify({"error": "No file provided"}), 400

        if not os.path.exists(file_path):
            return jsonify({"error": "File not found"}), 400

        # Load rules from 'rules' sheet
        try:
            df_rules = pd.read_excel(file_path, sheet_name='rules')
        except ValueError:
            return jsonify({"error": "Please generate rules first."}), 400

        # Validate rules data
        if df_rules.empty or 'Rule Name' not in df_rules.columns or 'Description' not in df_rules.columns:
            return jsonify({"error": "Please generate rules first."}), 400

        # Convert rules to JSON format
        rules = df_rules[['Rule Name', 'Description']].to_dict(orient='records')

        # Load main dataset (first sheet)
        df = pd.read_excel(file_path, sheet_name=0)

        # Construct filtering prompt for Gemini
        prompt = f"""
        You are an expert in data validation. Based on the following rules, filter the dataset and return only rows that match the criteria.

        Rules:
        {json.dumps(rules, indent=2)}

        Dataset:
        {df.head(10).to_dict(orient='records')}  # Sending first 10 rows for Gemini analysis

        Instructions:
        - Apply the rules to filter the dataset.
        - Keep all existing columns in the original dataset.
        - Return the results as JSON in the format: {{ "columns": [...], "data": [...] }}
        """

        # Call Gemini to process filtering
        response = model.generate_content(prompt)

        if response and response.candidates:
            try:
                # Extract JSON response from Gemini
                filtered_text = response.candidates[0].content.parts[0].text.strip()

                # Handle potential markdown artifacts
                if filtered_text.startswith("```json"):
                    filtered_text = filtered_text[7:].strip("```")


                filtered_data = json.loads(filtered_text)

                # Convert filtered data to DataFrame
                df_filtered = pd.DataFrame(filtered_data["data"])

                # Ensure all values are strings and strip whitespace
                df_filtered = df_filtered.fillna("").astype(str).apply(
                    lambda col: col.map(lambda x: x.strip() if isinstance(x, str) else x))

                # Prepare response format
                columns = df_filtered.columns.tolist()
                data = df_filtered.to_dict(orient='records')

                return jsonify({"columns": columns, "data": data})

            except json.JSONDecodeError as e:
                return jsonify({"error": f"JSON parsing error: {e}"}), 500

        return jsonify({"error": "No valid response from Gemini"}), 500

    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Function to check fuzzy match
def fuzzy_match(text, rules, threshold=50):
    if pd.isna(text) or not rules:
        return False
    best_match = process.extractOne(text, rules, scorer=fuzz.partial_ratio)
    return best_match and best_match[1] >= threshold  # Check if similarity is above threshold


@app.route('/filter-excel1', methods=['GET'])
def filter_excel1():
    try:
        # Load Excel file
        file_path = request.args.get('file') # Change this to your actual file path

        try:
            df_rules = pd.read_excel(file_path, sheet_name='rules')

        except ValueError:
            return jsonify({"error": "Please generate rules first."}), 400

        df = pd.read_excel(file_path,sheet_name=0)
        rules = df_rules.iloc[:, 1].dropna().astype(str).tolist()  # Extract second column as rules

        column_name1 = df.columns[2]
        column_name2 = df.columns[3]
        # Apply fuzzy matching
        filtered_df = df[df["Description"].apply(lambda x: fuzzy_match(x, rules))]

        output_file = "filtered_data.xlsx"
        filtered_df.to_excel(output_file, index=False)

        # Return the filtered data as JSON
        send_file(output_file, as_attachment=True)

        file_path = os.path.join(os.path.dirname(__file__), "filtered_data.xlsx")
        if not os.path.exists(file_path):
            print("File not found!")
        else:
            df1 = pd.read_excel(file_path, sheet_name=0)

        if df1.empty:
            return jsonify([])

        df1 = df1.fillna("").astype(str).apply(
            lambda col: col.map(lambda x: x.strip() if isinstance(x, str) else x))  # Replace NaN with empty string
        columns = df1.columns.tolist()
        data = df1.to_dict(orient='records')  # Convert data to JSON
        return jsonify({"columns": columns, "data": data})  # Send both columns and data

    except Exception as e:
        return jsonify({"error": str(e)}), 500
if __name__ == '__main__':
    app.run(port=5001, debug=True)
